---
title: "Beyond the average: individual variation is the main feature of Australia's PISA test results"
author: "Sarah Romanes and Di Cook"
date: "Jan 15, 2020"
output: html_document
---

![](figures/many_fish.jpg)

# Introduction

The PISA 2018 results were released on 3 December 2019. This led to wringing of hands in the Australian press, with titles of stories like [Vital Signs: Australia's slipping student scores will lead to greater income inequality](https://theconversation.com/vital-signs-australias-slipping-student-scores-will-lead-to-greater-income-inequality-128301) and [In China, Nicholas studied maths 20 hours a week. In Australia, it's three](https://www.smh.com.au/education/in-china-nicholas-studied-maths-20-hours-a-week-in-australia-it-s-three-20191203-p53ggv.html). Australia's neighbours, New Zealand and Indonesia, are also worrying: [New Zealand top-end in OECD's latest PISA report but drop in achievements 'worrying'](https://www.stuff.co.nz/national/education/117890945/new-zealand-topend-in-oecds-latest-pisa-report-but-drop-in-achievements-worrying), [Not even mediocre? Indonesian students score low in math, reading, science: PISA report](https://www.thejakartapost.com/news/2019/12/04/not-even-mediocre-indonesian-students-score-low-in-math-reading-science-pisa-report.html). 

The data from this survey and all of the surveys conducted since the first collection in 2000, is publicly available. We have made a more convenient subset of the data available in a new R package, called `learningtower`, along with sample code for analysis, openly available. What you can learn by looking at the data, is that the variation from individual is vastly dominates any difference between averages. Focusing attention on the averages, can be convenient, from a public policy perspective, it detracts from paying attenton to the individual, and boosting opportunities for all. 

# About the data

Since 2000, the OECD has overseen a Programme for International Student Assessment (PISA). The goal of this programme is to provide reliable indicators of what people actually know and can do. Details can be found at https://www.oecd.org/pisa/. Every three years, 15 year old students around the world are tested on standardised skills in mathematics, science and reading. The students are also surveyed on aspects of their everyday life that includes resources, quality of life and study habits. In addition, surveys are conducted with the school administrators and parents about resources and quality of life. 

The scope is gradually broadening, with more countries being added every three years. 2018 had results from 80 countries, which was double the initial number of 43. Each country is responsible for their data collection, under OECD oversight, and this is mostly consistent with a few notable exceptions, such as China and the USA. In Australia, for the 2018 survey, **???** schools were selected for testing of 14273 students. Stratified sampling was conducted to ensure representation from each state and territory, each type of school (public, private and catholic). A balance across urbanisation, socieoeconomic status, gender was checked.  An average of 20 students are tested from each school. 

China samples schools and students selectively, with OECD approval, and it is not entirely clear that the data is representative of the whole country. ([See a Washington Post article by Valerie Strauss for commentary on China's PISA practices](https://www.washingtonpost.com/education/2019/12/04/china-is-no-pisa-heres-why-its-test-scores-are-hard-believe/).) In the USA, the procedures for data collection are carefully documented and approved by the OECD, but the sample of schools and students is relatively small, at about a third of the numbers sampled in Australia. Schools can refuse to take part. (See [the technical notes for the USA PISA surveys for details](https://nces.ed.gov/pubs2011/2011004.pdf).) 

```{r loaddata, out.width="50%", echo = FALSE, fig.retina = 4, message = FALSE, warning = FALSE, cache=TRUE, fig.show="hide"}
library(tidyverse)
library(learningtower)
data(student)
```

```{r collection, out.width="50%", echo = FALSE, fig.retina = 4, message = FALSE, warning = FALSE, cache=TRUE, fig.show="hide", eval=FALSE}
student %>% group_by(year) %>%
  count(country) %>%
  count(year) %>%
  ggplot(aes(year, n)) + 
    geom_col(width=1) +
    xlab("") +
    ylab("Number of countries reporting")

student %>% filter(country == "AUS", year == 2018) %>% count(school_id, sort=TRUE)
student %>% filter(country == "AUS", year == 2018) %>% count(school_id) %>% summarise(n=sum(n))
student %>% filter(country == "AUS", year == 2018) %>% count(school_id) %>% select(n) %>% summary()
# Something isn't correct with 2015, 2018 school_id
```

The data from the testing, and the responses to the survey questions from students, parents and schools is freely available for [download](https://www.oecd.org/pisa/data/). It needs to be noted, though, that the raw test scores are not released. You can see this if you have a keen eye. The plots below show the 2018 scores for Australian students: a histogram of the math scores is plotted at left, and a scatterplot comparing math and reading scores is plotted at right. The histogram is beautifully bell-shaped and the scatterplot has a lovely elliptical appearance. These perfect structures should inspire suspician, and indeed if you read the data documentation you learn that the released values are simulated from a complex linear model applied to the data. Actually, multiple values are simulated for each student. This is called synthetic data, which is a common approach to ensure data privacy, and the data can still be considered to be accurate within the mean, variance and strata (country, gender, ...) used in the model of the original data.  

```{r scores, out.width="80%", fig.height=3, echo = FALSE, fig.retina = 4, message = FALSE, warning = FALSE, cache=TRUE}
library(gridExtra)
p1 <- student %>% filter(country == "AUS", year == 2018) %>%
  ggplot(aes(x=math)) + 
    geom_histogram(binwidth=30) + 
    xlim(c(0, 1000)) +
    xlab("Mathematics scores in Australia in 2018") +
    ylab("Count") + ggtitle("2018 Mathematics scores")
p2 <- student %>% filter(country == "AUS", year == 2018) %>%
  ggplot(aes(x=math, y=read)) + 
    geom_point(alpha=0.5) + 
    xlim(c(0, 1000)) + ylim(c(0, 1000)) +
    xlab("Mathematics scores in Australia in 2018") +
    ylab("Reading scores") +
    theme(aspect.ratio=1) + ggtitle("2018 Mathematics and Reading scores")
grid.arrange(p1, p2, ncol=2)
```

# Is it really this dramatic?

Richard Holden's Dec 6, 2019 article in the Conversation shows the figure below, along with an alarming statement "Australian 15-year-olds are now below the OECD average in mathematics, and our results in reading and science have fallen badly."

![](figures/conversation_holden.png)

A reason that this looks so dramatic, is that the scale of the differences is magnified. The repoorted scores range from 0 through 1000. These plots zoom in to the neighbourhood of 500. The animation below shows the average math scores on the larger scale of the data. For comparison several other countries are shown: Canada, Finland, Great Britain, Japan, New Zealand, Singapore and the USA. 

There has been a decline of 25 points in Australian students math scores on average, since 2000, which sounds bad, and particularly if the plot is made on a range of 450-550 also looks terrible. (It should be noted that the average for 2018 is similar to that for 2015.)
If these values are examined on the full data scale, the line looks flat, and particularly is not very different from related countries. All years the average is higher than the USA. Except for this past year, the averages were all higher than those in Great Britain. New Zealand averages are similar to Australia. Finland, the education shining star, has also seen a decline in average math score, but in contrast to Australian news headlines in Finland the title is [Finland Remains Among Top Nations in PISA Education Survey](https://finland.fi/life-society/finland-remains-among-top-nations-in-pisa-education-survey/).

All of the scores fluctuate, and it is arguable that the level of fluctuation is within sampling and modeling error.

<!--
Looking at the data in detail can show alarming fluctations. However when examined at a larger scale, the results are not concerning.

- difference in mean is exaggeraed
- lowest score in australa has gone up for reading
- focus on improving the low scorers
-->

```{r animation, echo = FALSE, fig.retina = 4, message = FALSE, warning = FALSE, cache=TRUE}
library(gganimate)

stu_sub <- student %>% 
  filter(country == "SGP" | country == "CAN"| country == "FIN" | country == "NZL" | country == "USA" | country == "JPN" | country == "GBR" | country == "AUS") %>% 
  group_by(year, country) %>%
  summarise(math = weighted.mean(math, stu_wgt, na.rm=TRUE))
s <- stu_sub %>%
  ggplot(aes(x=year, y=math, group=country, color = country)) + 
  geom_line() +
  geom_point() +
  geom_line(data=filter(stu_sub, country == "AUS"), size=2) +
  ylim(c(250,800)) + 
  theme_minimal() +
  ylab("Score") +
  xlab("Year")+
  theme(text = element_text(size=20)) +
  ggtitle("Mathematics PISA Scores from 2000 - 2018") +
  scale_color_brewer(palette = "Dark2") +
  view_zoom_manual(pause_length = 5,
                   step_length = 10, 
                   xmin = c(2000, 2018),
                   xmax = c(2000, 2018),
                   ymin = c(00, 450),
                   ymax = c(1000, 550),
                   wrap = FALSE,
                   fixed_x = TRUE)

gif <- animate(s, 
               device = "png",
                type="cairo",
                units="in", 
                width=9, 
                height=6, 
                pointsize=24, 
                res=200)

gif

```

# Exploring PISA

## Individuals

In the plot below, one dot represents one student. The scores for students in each country are very spread out: the lowest scores for each country are similar, and the highest scores for each country are similar. Even, though Singapore has a higher median score, the top math score in Australia is higher than the top math score in Singapore, and Canada has the highest individual overall math score. 

```{r individual, echo = FALSE, fig.retina = 4, message = FALSE, warning = FALSE, cache=TRUE}
library(ggbeeswarm)
library(forcats)
student %>% 
  filter(country == "SGP" | country == "CAN"| country == "FIN" | 
           country == "NZL" | country == "USA" | country == "JPN" | 
           country == "GBR" | country == "AUS") %>%
  filter(year == 2018) %>%
  ggplot(aes(x=fct_reorder(country, math), y=math)) + geom_quasirandom() +
      geom_violin(draw_quantiles=c(0.25, 0.5, 0.75), fill=NA) +
      ylab("Math scores") + xlab("") +
      ggtitle("Individual Student Mathematics PISA Scores from 2018")
```

## Gender gap

Plot of gender gap over time, in math and reading. 

```{r gender, echo = FALSE, fig.retina = 4, message = FALSE, warning = FALSE, cache=TRUE}
student %>% 
  filter(country == "AUS") %>%
  filter(gender %in% c(1,2)) %>%
  mutate(gender=factor(gender, levels=c(1,2), labels=c("f", "m"))) %>%
  ggplot(aes(x=gender, y=math)) + geom_quasirandom(aes(colour=gender)) +
      #geom_violin(draw_quantiles=c(0.25, 0.5, 0.75), aes(fill=gender), colour="white") +
      facet_wrap(~year, ncol=7) +
      ylab("Mathematics scores") + xlab("") +
      scale_colour_brewer("", palette="Dark2") +
      ggtitle("Individual Student Mathematics PISA Scores from 2000-2018")
student %>% 
  filter(country == "AUS") %>%
  filter(gender %in% c(1,2)) %>%
  mutate(gender=factor(gender, levels=c(1,2), labels=c("f", "m"))) %>%
  ggplot(aes(x=gender, y=read)) + geom_quasirandom(aes(colour=gender)) +
      #geom_violin(draw_quantiles=c(0.25, 0.5, 0.75), aes(fill=gender), colour="white") +
      facet_wrap(~year, ncol=7) +
      ylab("Reading scores") + xlab("") +
      scale_colour_brewer("", palette="Dark2") +
      ggtitle("Individual Student Reading PISA Scores from 2000-2018")
```

Something's off with gender - cross-checking against the PISA Australia summary, and it doesn't look correct here.

```{r gendergap, eval=FALSE, echo = FALSE, fig.retina = 4, message = FALSE, warning = FALSE, cache=TRUE}
gender <- student %>% 
  filter(country == "AUS") %>% 
  filter(gender %in% c(1,2)) %>%
  mutate(gender=factor(gender, levels=c(1,2), labels=c("f", "m"))) %>%
  group_by(gender, year) %>%
  summarise(math = weighted.mean(math, stu_wgt, na.rm=TRUE), 
            read = weighted.mean(read, stu_wgt, na.rm=TRUE)) %>%
  gather(topic, score, math, read)
ggplot(gender, aes(x=year, y=score, colour=gender)) + geom_line() + facet_wrap(~topic, ncol=2) +
  scale_color_brewer("", palette="Dark2")
```

<!--XX Can't do this for now - data not available for homework Homework time vs scores-->

# Do it yourself

The a subset of variables from the PISA data across all years is provided in a convenient format in the R package, `learningtower` available from https://github.com/kevinwang09/learningtower. Details on how to install the package, and access the data are at https://kevinwang09.github.io/learningtower/. 

The work to make the data available is the effort of several researchers from Australia, New Zealand and Indonesia, conducted as part of the [ROpenSci OzUnconf](https://ozunconf19.ropensci.org) held in Sydney, Dec 11-13, 2019.
